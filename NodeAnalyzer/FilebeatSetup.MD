# How to Analyze on offline BEK (Beats/Filebeat ElasticSearch Kibana) Setup

When you get a dump of the logs after unzipping all of the gc and log files, use this filebeat template to ingest the logs using a parser that is specialized
for this type of dump. 

This doc does not show you how to setup Filebeat / Kibana / Elastic but assumes you know how to do that. This is just focused on configuring filebeat.

## prepare by unzipping the files 

In the main directory with the host names as different folders , this bash script will unzip everything if it is rolled up. 

```
for z in * ; do echo $z ; cd $z/cassandra ; unzip '*.zip' ; cd ../../ ; done
```

May have to enter "A" or "Y" to override, etc. 


## fields.yml (usually sits in /etc/filebeat/)
This was added to the fields.yml but probably don't need it. 
```
.....
- key: cassandra
  title: Cassandra
  description: >
    Module for parsing Cassandra system log.
  fields:
    - name: ingest
    type: group
    description: >
      Parsed values of the ingestion tasks.
    fields:
    - name: timestamp
      type: date
      description: >
        The actual timestamp.
    - name: loglevel
      type: keyword
      description: >
        The loglevel.
    - name: message
      type: text
      description: >
        The actual message.
 ```

## filebeat.yml (usually sits in /etc/filebeat/)
```
###################### Filebeat Configuration Example #########################

#=========================== Filebeat inputs =============================

filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /home/anant/Projects/Client/ClientName/incident.2020060919/Hostname1/cassandra/system.log*
      - /home/anant/Projects/Client/ClientName/incident.2020060919/Hostname2/cassandra/system.log*
    exclude_files: ['\.zip$']  
    multiline.pattern: ^TRACE|DEBUG|WARN|INFO|ERROR
    multiline.negate: true
    multiline.match: after
  - type: log
    enabled: true
    paths:
      - /home/anant/Projects/Client/ClientName/incident.2020060919/Hostname1/cassandra/gc.log*
      - /home/anant/Projects/Client/ClientName/incident.2020060919/Hostname2/cassandra/gc.log*
    exclude_files: ['\.zip$']
    multiline.pattern: ^TRACE|DEBUG|WARN|INFO|ERROR
    multiline.negate: true
    multiline.match: after

#==================== Elasticsearch template setting ==========================
setup.template.settings:
  index.number_of_shards: 1
  #index.codec: best_compression
  _source.enabled: false
#================================ General =====================================
#name:
#tags: ["service-X", "web-tier"]
#fields:
#  env: staging

#============================== Dashboards =====================================
setup.dashboards.enabled: true
#setup.dashboards.url:

#============================== Kibana =====================================
setup.kibana:
#================================ Outputs =====================================
#-------------------------- Elasticsearch output ------------------------------
output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["localhost:9200"]

  # Protocol - either `http` (default) or `https`.
  #protocol: "https"

  # Authentication credentials - either API key or username/password.
  #api_key: "id:api_key"
  #username: "elastic"
  #password: "changeme"

#----------------------------- Logstash output --------------------------------
#output.logstash:
  # The Logstash hosts
  #hosts: ["localhost:5044"]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"

#================================ Processors =====================================
# Configure processors to enhance or manipulate events generated by the beat.

processors:
  # ( since we are not using a live host.. these are irrelevant) 
  #- add_host_metadata: ~ 
  #- add_cloud_metadata: ~
  #- add_docker_metadata: ~ 
  #- add_kubernetes_metadata: 
 
  - dissect:
      tokenizer: "%{loglevel} [%{component}] %{timestamp} %{+timestamp},%{} %{class}.%{}:%{} - %{message}"
      field: "message"
      target_prefix: "ingest"
  - dissect:
      tokenizer: "%{componentname}:%{}"
      field: "ingest.component"
      target_prefix: "ingest"
  - dissect:
      tokenizer: "%{streamdirection}/%{streamvector}"
      field: "ingest.component"
      target_prefix: "ingest"
  - dissect:
      tokenizer: "/%{}/%{}/%{}/%{}/%{}/cass.stat.%{incident-id}/%{host-name}/cassandra/"
      field: "log.file.path"
      target_prefix: "ingest"
  - timestamp:
      field: "ingest.timestamp"
      layouts:
        - '2006-01-02 15:04:05'
      test:
        - '2019-06-22 16:33:51'
        - '2020-06-13 12:57:30'
      ignore_failure: true
      target_field: "@timestamp"
  - include_fields:
      fields: [ "ingest", "message","log","input" ]


 
```
